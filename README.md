# NN_Project
## Methods
Our system aims to recommend League of Legends champions based on a user’s natural language description of their preferred playstyle. To achieve this, we will create a semantic matching pipeline that compares user input to champion profiles derived from official ability descriptions and community-sourced gameplay guides.
### Data Collection
We gathered champion-related text data from multiple sources to build a rich, descriptive profile for each champion. Official Riot Games API data for League of Legends provided information such as champion names, titles, lore, abilities, stats, skins, ally and enemy counters, and roles. However, because this official data was often outdated, we also web-scraped the League of Legends Wiki to collect more up-to-date champion information. The wiki contained detailed champion names, statistics, abilities, and, importantly, updated position information indicating which roles each champion is commonly played in.
To gain deeper insight into champion gameplay styles, we gathered text data from community guides by scraping Mobafire. Mobafire provides detailed strategy and build guides written by players, offering nuanced descriptions of each champion’s strengths, weaknesses, and typical playstyles. These community-driven resources helped supplement the official and wiki-sourced data by providing more personalized insights into how champions are played in practice.
In addition to full text descriptions, we collected metadata such as champion tags and labels. Position labels, such as "Top," "Jungle," "Mid," "ADC," and "Support," were used to filter champions during recommendation based on the player's specified preferred role. Playstyle tags, such as "Mage," "Assassin," "Tank," "Marksman," "Fighter," and "Support," were incorporated into the text descriptions to provide additional context about each champion’s typical gameplay style. These tags helped enrich the embeddings, allowing the model to better associate natural language inputs with champions that matched the desired playstyle.
Finally, we combined all collected text data into a single cleaned description per champion. The cleaning process involved removing stopwords, converting all text to lowercase, and eliminating special characters to standardize the inputs for embedding.
### Model Building
To compare natural language within the user input and the champion descriptions, we chose to embed the sentence-level text data because of the flexibility that embedding provides when it comes to unsupervised, unlabeled language data. Multiple pre-trained models can be used for embedding the user input and champion descriptions that were acquired in the data collection phase. These pre-trained models can be used to assist in applying semantic similarity techniques comparing user input to the champion description embeddings at a sentence level. The SentenceTransformers pre-trained model, also known as SBERT, is used for sentence-level embeddings on both the user input and champion description and for calculating a similarity score between the two. SBERT is a popular way of embedding this type of data, and it can also provide opportunities for fine-tuning. OpenAI embeddings can also be used to calculate a measurement for the similarity at the sentence level. Depending on the performance of these pre-trained models, we can use a MiniLM, which is a model that uses self-attention distillation to improve the efficiency of pre-models like BERT, to reduce runtime without affecting accuracy. SBERT can be improved by using labeled prompts with labels that can be directly matched to a champion, which we used the position label for. Passing the champion description and user input through the embedding model will generate fixed-length vectors that can then be used to calculate cosine similarity measurements. Cosine similarity is a popular measurement of closeness and will give us a value used to evaluate the similarity between the user’s input and each champion’s description. 
### Recommendation Pipeline
To compare natural language within the user input and the champion descriptions, we chose to embed the sentence-level text data using SBERT. First, the champion descriptions are embedded using an NLP model, creating vector representations for each champion’s combined text profile. After embedding the champion descriptions, the user is prompted to enter a short description, typically one to two sentences, of their preferred playstyle, along with a required preferred position. An example input would be, “I enjoy long-range mages who can poke from a distance and control teamfights,” with the position specified as "Mid." Once the user input is received, it is embedded using the same NLP model used for the champion descriptions, transforming the user input into a comparable vector.
After embedding, champion filtering based on the player’s specified position is applied. Using metadata labels such as "Mid," "Jungle," or "Support," the champion pool is restricted to only those champions associated with the chosen role. Next, cosine similarity is calculated between the user input vector and the vectors for the remaining filtered champion descriptions. This produces a similarity score for each champion, where a higher score represents a greater closeness between the user’s described playstyle and the champion’s profile. Finally, the model ranks all champions based on their similarity scores and returns the champions with the top-k highest similarity scores as the final recommendations.
### System Specifications
The pipeline was implemented in PyTorch using the Python programming language. It used the transformers library for sentence embeddings (specifically Sentence-BERT), pandas and NumPy for data manipulation and preprocessing, and scikit-learn for calculating cosine similarities. Computations were performed on local machines, and GPU acceleration was used when available to speed up embedding generation and similarity scoring. All code was organized in a Jupyter Notebook and is available in a GitHub repository, accompanied by a README file that includes model specifications and instructions for reproducibility.
## Experiment 1: Prompt-Based Similarity Evaluation
### Purpose
The purpose of Experiment 1 was to test whether our model could correctly match natural language playstyle descriptions to appropriate champions, filtered by the specified role, based on similarity scoring.
### Methods
We created a dataset containing 15 prompts representing common playstyles, such as aggressive ADCs, tanky top laners, or control mages in the mid lane. Each prompt consisted of one to three sentences describing a desired playstyle and a specified preferred position. For example, a prompt could be, "I like durable champions who initiate fights," with "Top" as the chosen role. We manually assigned ten ground-truth champions to each prompt based on champion descriptions and gameplay roles. Each prompt was input into the model, where champion descriptions were embedded using SBERT, the user input was embedded with the same model, champions were filtered based on the indicated role, and cosine similarity scores were calculated between the user input and the filtered champion profiles. Champions were then ranked based on similarity scores to generate the top-k recommendations for each prompt.
### Evaluation Metrics
We evaluated the model using two metrics: Top-10 Accuracy and Mean Reciprocal Rank (MRR). Top-10 Accuracy measured whether at least one ground truth champion appeared among the top ten recommended champions. Mean Reciprocal Rank assessed how highly ranked the first correct champion was among the recommendations.
## Experiment 2: User Preference Evaluation
### Purpose
The purpose of Experiment 2 was to assess whether real players found the system’s recommendations to be relevant and aligned with their described playstyle and specified role, providing insight into the model's practical effectiveness.
### Methods
We recruited 12 participants, including both new and experienced League of Legends players. Each participant wrote a one- to two-sentence description of their preferred playstyle along with a required position, such as Mid, Jungle, Top, ADC, or Support. An example input was, "I want a jungler who is tanky and has a lot of crowd control," with "Jungle" as the specified role. We processed each input through the model, embedded the user input using SBERT, filtered the champion pool based on the participant's specified role, calculated cosine similarity scores, and generated a list of top-k recommended champions. Participants completed a survey where they rated the quality of the recommendations using a 5-point Likert scale and provided open-ended feedback about whether the recommendations helped them discover new champions.
### Evaluation Metrics
We evaluated the model based on the mean satisfaction score and Precision@k metrics. The mean satisfaction score was calculated from participant responses on a 5-point Likert scale. Precision@1 measured the percentage of users who found the top recommendation a strong match, while Precision@5 captured the percentage of users who found at least one good match among the top five recommendations.
